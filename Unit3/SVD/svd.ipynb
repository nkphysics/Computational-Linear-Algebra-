{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79217dc0-9a1b-4867-9037-a4ba3b11401d",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Singular Value Decomposition of an $m \\times n$ matrix $\\mathbf{A}$ is a factorization of the form:\n",
    "\n",
    "$\\mathbf{A}_{m \\times n} = \\mathbf{U} \\Sigma \\mathbf{V}^T$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\mathbf{U}$ is an $m \\times r$ orthonormal matrix, where $r$ is the rank of $\\mathbf{A}$.\n",
    "* $\\Sigma$ is an $r \\times r$ diagonal matrix containing the singular values of $\\mathbf{A}$.\n",
    "* $\\mathbf{V}$ is an $n \\times r$ orthonormal matrix.\n",
    "\n",
    "**Notable Properties**\n",
    "\n",
    "* The matrices $\\mathbf{U}, \\Sigma,$ and $\\mathbf{V}$ are unique, except for possible permutations of the columns of $\\mathbf{U}$ and \n",
    "$\\mathbf{V}$.\n",
    "* The singular values in $\\Sigma$ are non-negative and sorted in descending order: $\\sigma_1 \\geq \\ldots \\geq \\sigma_r > 0$.\n",
    "* The matrices $\\mathbf{U}$ and $\\mathbf{V}$ have orthonormal columns: $\\mathbf{U}^T \\mathbf{U} = \\mathbf{I}$ and $\\mathbf{V}^T \n",
    "\\mathbf{V} = \\mathbf{I}$.\n",
    "\n",
    "The SVD and its corresponding singular values of $\\Sigma$ hold significant value due to their analogous nature to eigenvalues. While eigenvalues are a fundamental concept in linear algebra, speaking from the world of numerical/computational mathematics, they are only reliable when working with square matrices. \n",
    "In contrast, singular values can be used with matrices of any size, making them a powerful tool (notice that above I have specified that $\\mathbf{A}$ is $m \\times n$ in size). This is particularly relevant in the Data Science world, where data rarely comes in perfectly square matrices. As a result, singular values offer a more flexible and versatile alternative to eigenvalues, allowing practitioners to extract meaningful insights from data sets. As far as real world use goes the SVD and singular values are used in many real world applications. Below are just a few examples of some of the applications.\n",
    "\n",
    "**Real World Applications of SVD**\n",
    "\n",
    "* Dimensionality reduction: SVD can be used to reduce the dimensionality of a dataset while preserving most of the information.\n",
    "* Computing the pseudoinverse of a matrix\n",
    "* Data compression: SVD can be used to compress a matrix by retaining only the top few singular values and their corresponding columns in \n",
    "$\\mathbf{U}$ and $\\mathbf{V}$.\n",
    "* Image processing: SVD is often used in image processing algorithms, such as image denoising and deblurring.\n",
    "\n",
    "So if the singular values and the $\\Sigma$ matrix in particular are so useful, let's start there and discuss how we can compute them.\n",
    "\n",
    "## Computing the Singular Values\n",
    "\n",
    "**Step 1: Compute a Eigenvalues for $\\mathbf{A}^T \\mathbf{A}$**\n",
    "\n",
    "The first step is to compute the eigenvalues ($\\lambda$'s) of the matrix $\\mathbf{A}^T \\mathbf{A}$, where $\\mathbf{A}$ is a $m \\times n$ matrix. This can be done because $\\mathbf{A}^T \\mathbf{A}$ will be square (i.e. $m \\times m$ dimensions). There are a variety of methods for computing eigenvalues and I'll refer you to the notebook on computing eigenvalues for examples of such methods. \n",
    "\n",
    "**Step 2: From the Eigenvalues Compute the Singular Values**\n",
    "\n",
    "After computing the eigenvalues, we can compute the singular values of $\\mathbf{A}$ by simply taking the square roots of the eigenvalues.\n",
    "\n",
    "$$\\sigma_i = \\sqrt{\\lambda_i},$$for $i=1,\\ldots,r$..\n",
    "\n",
    "The matrix of singular values ($\\Sigma$) can then be constructed by the following:\n",
    "\n",
    "$$\\Sigma = \\begin{bmatrix}\n",
    "\\sigma_1 & 0 & 0 & 0 \\\\\n",
    "0 & \\sigma_2 & 0 & 0 \\\\\n",
    "0 & 0 & \\ddots & 0 \\\\\n",
    "0 & 0 & 0 & \\sigma_r\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "To see that this is the case, lets test it out using the `numpy.linalg.eig()` and `numpy.linalg.svd()` functions in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bf9bc62-1b38-4961-ab4d-64957bee65cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[  53.  -86.   99.  -46.]\n",
      " [  32.   47.   87.   43.]\n",
      " [  10.   85.   94.   72.]\n",
      " [ -88.   -3.  -26.   50.]\n",
      " [ -77. -100.   56.  -15.]\n",
      " [  -2.   73.   50.   39.]\n",
      " [  98.  -56.  -63.   96.]\n",
      " [ -88.  -73.  -85.   25.]\n",
      " [  10.  -50.   96.  -81.]\n",
      " [ -50.  -26.   -5.   64.]]\n",
      "AT A:\n",
      " [[ 37558.   7350.   9363.   -467.]\n",
      " [  7350.  43809.   6756.  11479.]\n",
      " [  9363.   6756.  52953. -10504.]\n",
      " [  -467.  11479. -10504.  33893.]]\n",
      "Singular Values for A transpose A: \n",
      " [250.36820574 228.98786071 178.11787873 146.17572466]\n",
      "Diagonal of Sigma Matrix from decomposition: \n",
      " [250.36820574 228.98786071 178.11787873 146.17572466]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random 10x4 A matrix with values between -100 and 100\n",
    "A = np.array(np.random.randint (-100, 100, (10, 4)), dtype=np.float64)\n",
    "print(f\"A:\\n {A}\")\n",
    "# Compute A transpose A\n",
    "ATA = np.dot(A.T, A)\n",
    "print(f\"AT A:\\n {ATA}\")\n",
    "# Compute the eigenvalues and eigenvectors of A transpose A\n",
    "ATA_eigvals, ATA_eigvecs = np.linalg.eig(ATA)\n",
    "# Take the square root of the eigenvalues to compute the singular values\n",
    "ATA_sigmas = np.sqrt(ATA_eigvals)\n",
    "print(f\"Singular Values for A transpose A: \\n {ATA_sigmas}\")\n",
    "\n",
    "# Confirm the singular values\n",
    "U, Sigma, VT = np.linalg.svd(A, full_matrices=False)\n",
    "print(f\"Diagonal of Sigma Matrix from decomposition: \\n {Sigma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506eb87a-cb5c-4095-bcf1-40e6fadc148a",
   "metadata": {},
   "source": [
    "Notice that the singular values, when computing the eigenvalues and then taking the square root of them, are equivalent to what we get along the diagonal of $\\Sigma$ when we compute a SVD. Also note that the singular values are the same for $\\mathbf{A}^T \\mathbf{A}$ are the same as for $\\mathbf{A}$.\n",
    "\n",
    "## Computing $\\mathbf{U}$ and $\\mathbf{V}$\n",
    "\n",
    "To get $\\mathbf{V}^T$ just compute the eigenvectors of $A^T A$ and to form the columns of the $\\mathbf{V}^T$ matrix. This works because if $\\mathbf{A}_{m \\times n} = \\mathbf{U} \\Sigma \\mathbf{V}^T$, then...\n",
    "\n",
    "$\\mathbf{A}^T \\mathbf{A}  = \\mathbf{V} \\Sigma \\mathbf{U}^T \\mathbf{U} \\Sigma \\mathbf{V}^T = \\mathbf{V} {\\Sigma}^T \\Sigma \\mathbf{V}^T = \\mathbf{V} {\\Sigma}^2 \\mathbf{V}^T$ \n",
    "\n",
    "Remember $\\mathbf{U}$ and $\\mathbf{V}$ are have orthonormal columns. That is why the $\\mathbf{U}^T \\mathbf{U}$ cancels. The ${\\Sigma}^2$ also helps explain the relationship between the eigenvalues and singular values. We can see this by examining the derivation of the Eigendecomposition for some square $A$:\n",
    "\n",
    "$A \\vec{v} = \\lambda \\vec{v}$\n",
    "\n",
    "$AQ = Q\\lambda$\n",
    "\n",
    "$A = Q \\Lambda Q^{-1}$ (Where $\\Lambda$ is a diagonal matrix with the eigenvalues along the diagonal)\n",
    "\n",
    "Even though we did not directly discuss the Eigendecomposition before in the notes on computing eigenvalues, hopefully (if you've seen those notes) you can see we discussed around the topic of the Eigendecomposition. Hopefully you can also see that $A$ and $\\Lambda$ are similar matrices. \n",
    "\n",
    "But remember, for this to work we need a square $A$. We accomplish that for the SVD by computing $A^T A$, so we can get the eigenvalue of $A^T A$. So if we stick everything next to one another we can see the following:\n",
    "\n",
    "$A^T A = Q \\Lambda Q^{-1}$\n",
    "\n",
    "$\\mathbf{A}^T \\mathbf{A} = \\mathbf{V} {\\Sigma}^2 \\mathbf{V}^T$\n",
    "\n",
    "Our orthonormal $Q$ and $V$ matrices line up nicely. So does $\\Lambda$ and ${\\Sigma}^2$. This tells us that the eigenvalues for $A^T A$ or $\\Lambda = {\\Sigma}^2$. So if we just want the singular values or $\\Sigma$ the we just need to take the square root of each eigenvalue for $A^T A$. Similarly if we want to get $V^T$ then we just need to get the eigenvectors.\n",
    "\n",
    "Then once we have $\\Sigma$ and $V^T$we can compute $U$ easily by the following:\n",
    "\n",
    "$U_i = \\frac{A V_i} {\\sigma_i}$\n",
    "\n",
    "Or more efficiently:\n",
    "\n",
    "$U = A V \\Sigma^{-1}$\n",
    "\n",
    "Normally inverting matrices should be avoided unless the matrices are orthonormal of diagonal. In this case $\\Sigma$ is diagonal, so inverting it is easy since the inverse will also be diagonal, and the values along the diagonal for the inverse of the original matrix will just be $\\frac{1} {\\sigma_i}$. \n",
    "\n",
    "With the following code, we'll test out computing $V^T$, $U$, and also ensure that when we compute $U \\Sigma V^T$ we get back A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea73b4a0-90cb-409f-a6d7-8d20383e1606",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvectors of A transpose A (the columns): \n",
      "[[ 0.41323731  0.35471767  0.82225214 -0.16526257]\n",
      " [ 0.16681964  0.72870496 -0.27685375  0.60374854]\n",
      " [-0.89354592  0.26826585  0.35015476  0.08367062]\n",
      " [ 0.05460608 -0.52076664  0.35305108  0.7753549 ]]\n",
      "\n",
      " V transpose matrix from np.linalg.svd: \n",
      "[[-0.41323731 -0.35471767 -0.82225214  0.16526257]\n",
      " [-0.16681964 -0.72870496  0.27685375 -0.60374854]\n",
      " [ 0.89354592 -0.26826585 -0.35015476 -0.08367062]\n",
      " [-0.05460608  0.52076664 -0.35305108 -0.7753549 ]]\n",
      "\n",
      "U from direct computation:\n",
      " [[ 0.32113062 -0.47604331 -0.22239387  0.32129675]\n",
      " [ 0.376745    0.18106756  0.10148519  0.28272182]\n",
      " [ 0.39811833  0.35396531  0.29646642  0.3098548 ]\n",
      " [-0.26788833  0.08960904  0.40931742  0.1802309 ]\n",
      " [-0.07495353 -0.48157857  0.33870861  0.38318629]\n",
      " [ 0.23858973  0.27322552  0.2365927   0.06681149]\n",
      " [-0.18785941  0.22246809 -0.65472236  0.59316401]\n",
      " [-0.54432738 -0.12773301  0.17616     0.15450692]\n",
      " [ 0.31441278 -0.48146064  0.02520121 -0.01591543]\n",
      " [-0.17802816  0.05562244  0.23190558  0.4013463 ]]\n",
      "\n",
      "U from np.linalg.svd:\n",
      " [[-0.32113062  0.47604331  0.22239387 -0.32129675]\n",
      " [-0.376745   -0.18106756 -0.10148519 -0.28272182]\n",
      " [-0.39811833 -0.35396531 -0.29646642 -0.3098548 ]\n",
      " [ 0.26788833 -0.08960904 -0.40931742 -0.1802309 ]\n",
      " [ 0.07495353  0.48157857 -0.33870861 -0.38318629]\n",
      " [-0.23858973 -0.27322552 -0.2365927  -0.06681149]\n",
      " [ 0.18785941 -0.22246809  0.65472236 -0.59316401]\n",
      " [ 0.54432738  0.12773301 -0.17616    -0.15450692]\n",
      " [-0.31441278  0.48146064 -0.02520121  0.01591543]\n",
      " [ 0.17802816 -0.05562244 -0.23190558 -0.4013463 ]]\n",
      "\n",
      "Recomputing A to check SVD:\n",
      " [[  53.  -86.   99.  -46.]\n",
      " [  32.   47.   87.   43.]\n",
      " [  10.   85.   94.   72.]\n",
      " [ -88.   -3.  -26.   50.]\n",
      " [ -77. -100.   56.  -15.]\n",
      " [  -2.   73.   50.   39.]\n",
      " [  98.  -56.  -63.   96.]\n",
      " [ -88.  -73.  -85.   25.]\n",
      " [  10.  -50.   96.  -81.]\n",
      " [ -50.  -26.   -5.   64.]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Eigenvectors of A transpose A (the columns): \\n{ATA_eigvecs.T}\\n\")\n",
    "print(f\" V transpose matrix from np.linalg.svd: \\n{VT}\\n\")\n",
    "comp_U = np.dot(np.dot(A, ATA_eigvecs), (1/ Sigma) * np.identity(len(Sigma)))\n",
    "print(f\"U from direct computation:\\n {comp_U}\\n\")\n",
    "print(f\"U from np.linalg.svd:\\n {U}\\n\")\n",
    "comp_svd = np.dot(np.dot(comp_U, np.identity(len(Sigma)) * Sigma), ATA_eigvecs.T)\n",
    "print(f\"Recomputing A to check SVD:\\n {comp_svd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d0676-2847-4ca5-a786-7df01c52ed5a",
   "metadata": {},
   "source": [
    "## SVD Considerations for Non-Square Matrices\n",
    "\n",
    "What about if we try to use $\\mathbf{A} \\mathbf{A}^T$ to compute a SVD? Let's experiment with what happeens when try to it to compute the same singular values with the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "621bb58a-4dd5-4497-8796-a37975c9dd16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AT A:\n",
      " [[ 2.2122e+04  4.2890e+03 -7.8600e+02 -9.2800e+03  1.0753e+04 -3.2280e+03\n",
      "  -6.4300e+02 -7.9510e+03  1.8060e+04 -3.8530e+03]\n",
      " [ 4.2890e+03  1.2651e+04  1.5589e+04 -3.0690e+03 -2.9370e+03  9.3940e+03\n",
      "  -8.4900e+02 -1.2567e+04  2.8390e+03 -5.0500e+02]\n",
      " [-7.8600e+02  1.5589e+04  2.1345e+04  2.1000e+01 -5.0860e+03  1.3693e+04\n",
      "  -2.7900e+03 -1.3275e+04 -9.5800e+02  1.4280e+03]\n",
      " [-9.2800e+03 -3.0690e+03  2.1000e+01  1.0929e+04  4.8700e+03  6.0700e+02\n",
      "  -2.0180e+03  1.1423e+04 -7.2760e+03  7.8080e+03]\n",
      " [ 1.0753e+04 -2.9370e+03 -5.0860e+03  4.8700e+03  1.9290e+04 -4.9310e+03\n",
      "  -6.9140e+03  8.9410e+03  1.0821e+04  5.2100e+03]\n",
      " [-3.2280e+03  9.3940e+03  1.3693e+04  6.0700e+02 -4.9310e+03  9.3540e+03\n",
      "  -3.6900e+03 -8.4280e+03 -2.0290e+03  4.4800e+02]\n",
      " [-6.4300e+02 -8.4900e+02 -2.7900e+03 -2.0180e+03 -6.9140e+03 -3.6900e+03\n",
      "   2.5925e+04  3.2190e+03 -1.0044e+04  3.0150e+03]\n",
      " [-7.9510e+03 -1.2567e+04 -1.3275e+04  1.1423e+04  8.9410e+03 -8.4280e+03\n",
      "   3.2190e+03  2.0923e+04 -7.4150e+03  8.3230e+03]\n",
      " [ 1.8060e+04  2.8390e+03 -9.5800e+02 -7.2760e+03  1.0821e+04 -2.0290e+03\n",
      "  -1.0044e+04 -7.4150e+03  1.8377e+04 -4.8640e+03]\n",
      " [-3.8530e+03 -5.0500e+02  1.4280e+03  7.8080e+03  5.2100e+03  4.4800e+02\n",
      "   3.0150e+03  8.3230e+03 -4.8640e+03  7.2970e+03]]\n",
      "\n",
      "Singular Values for A A transpose: \n",
      " [2.50368206e+02+0.00000000e+00j 2.28987861e+02+0.00000000e+00j\n",
      " 1.78117879e+02+0.00000000e+00j 1.46175725e+02+0.00000000e+00j\n",
      " 0.00000000e+00+2.98270933e-06j 1.90036046e-06+0.00000000e+00j\n",
      " 5.94041880e-07+7.68283819e-07j 5.94041880e-07-7.68283819e-07j\n",
      " 1.24283101e-06+0.00000000e+00j 0.00000000e+00+1.50155512e-06j]\n"
     ]
    }
   ],
   "source": [
    "# Compute A A transpose\n",
    "AAT = np.dot(A, A.T)\n",
    "print(f\"AT A:\\n {AAT}\\n\")\n",
    "# Compute the eigenvalues and eigenvectors of A A transpose\n",
    "AAT_eigvals, AAT_eigvecs = np.linalg.eig(AAT)\n",
    "# Take the square root of the eigenvalues to compute the singular values\n",
    "AAT_sigmas = np.sqrt(AAT_eigvals)\n",
    "print(f\"Singular Values for A A transpose: \\n {AAT_sigmas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a8e6f-d60f-47a4-9ba7-375acfbf1b26",
   "metadata": {},
   "source": [
    "That does not work out nicely. First, notice how large $\\mathbf{A} \\mathbf{A}^T$ is. It is a $10 \\times 10$ matrix, so right away this is more computationally intensive than $\\mathbf{A}^T \\mathbf{A}$, which is of smaller dimension. More importantly, notice that there are 10 singular values. In fact, some of these singular values even contain an imaginary component. Even though the first 4 $\\sigma_i$ agree with the 4 singular values we computed before for $\\mathbf{A}^T \\mathbf{A}$ and we could throw away all the complex $\\sigma_i$, we don't want to complicate things with complex numbers if we can avoid it. More importantly from the computational/numerical perspective, we don't want to have to compute more than we have to.\n",
    "\n",
    "This brings up an important point, one which has been addressed with previous notes/examples when working with non-square matrices. The point is that paying attention to the dimensions of the matrices we work with matters. Specifically, when computing a singular value decomposition (SVD) for a non-square matrix, the choice of whether to use $\\mathbf{A}^T \\mathbf{A}$ or $\\mathbf{A} \\mathbf{A}^T$ depends crucially on the dimensions of the matrix we want to decompose. If we have a matrix with more rows than columns ($m > n$), as is often the case in machine learning applications (data tables), it is more efficient and numerically stable to use $\\mathbf{A}^T \\mathbf{A}$, since this reduces the dimensions of the matrices we will be working with and thus avoids unnecessary computations. On the other hand, if we have a matrix with more columns than rows($m < n$), as may be the case in other applications, it is better to use $\\mathbf{A} \\mathbf{A}^T$ for the same reasons. Put more bluntly, if we have the option to work with smaller matrices, we generally always want to make the choice of that option.\n",
    "\n",
    "Let's see this work for a new matrix $B$ that is ($4 \\times 10$) in dimension with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d83e349-8bfd-43ba-83d4-1b56caea19d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B:\n",
      " [[ 68. -70. -16.  44. -17. -53.  16.  99.  91. -96.]\n",
      " [-38.  -2. -75. -19. -16.  13. -46.  18. -33. -66.]\n",
      " [-46. -11. -49. -49.  45.  33.   8.  46. -87. -87.]\n",
      " [-71.  25.  51. -34. -20. -95. -27.  77.  51. -88.]]\n",
      "B BT:\n",
      " [[42368.  1882. -1127. 16765.]\n",
      " [ 1882. 15744. 15158.  5307.]\n",
      " [-1127. 15158. 27471.  4668.]\n",
      " [16765.  5307.  4668. 35851.]]\n",
      "Singular Values for A transpose A: \n",
      " [239.5795294  195.97855783 142.92731685  72.1098888 ]\n",
      "Diagonal of Sigma Matrix from decomposition: \n",
      " [239.5795294  195.97855783 142.92731685  72.1098888 ]\n",
      "Eigenvectors of B B transpose (the columns): \n",
      "[[ 0.72773256  0.36452168 -0.57936363  0.04320937]\n",
      " [ 0.17343585 -0.50792361 -0.16345862 -0.82777708]\n",
      " [ 0.16085439 -0.77622945 -0.2446978   0.55831595]\n",
      " [ 0.64377884 -0.08127359  0.76009344  0.03466064]]\n",
      "\n",
      "U matrix from np.linalg.svd: \n",
      "[[-0.72773256  0.36452168 -0.57936363  0.04320937]\n",
      " [-0.17343585 -0.50792361 -0.16345862 -0.82777708]\n",
      " [-0.16085439 -0.77622945 -0.2446978   0.55831595]\n",
      " [-0.64377884 -0.08127359  0.76009344  0.03466064]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate random 4x10 A matrix with values between -100 and 100\n",
    "B = np.array(np.random.randint (-100, 100, (4, 10)), dtype=np.float64)\n",
    "print(f\"B:\\n {B}\")\n",
    "BBT = np.dot(B, B.T)\n",
    "print(f\"B BT:\\n {BBT}\")\n",
    "BBT_eigvals, BBT_eigvecs = np.linalg.eig(BBT)\n",
    "BBT_sigmas = np.sqrt(BBT_eigvals)\n",
    "print(f\"Singular Values for A transpose A: \\n {BBT_sigmas}\")\n",
    "\n",
    "# Confirm the singular values\n",
    "B_U, B_Sigma, B_VT = np.linalg.svd(B, full_matrices=False)\n",
    "print(f\"Diagonal of Sigma Matrix from decomposition: \\n {B_Sigma}\")\n",
    "# Confirm U\n",
    "print(f\"Eigenvectors of B B transpose (the columns): \\n{BBT_eigvecs}\\n\")\n",
    "print(f\"U matrix from np.linalg.svd: \\n{B_U}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d211f-e851-47ab-9ab9-1f4cbe8e2192",
   "metadata": {},
   "source": [
    "As you can see, the computation of singular values using $A A^T$ for a matrix with more columns than rows works out really nicely since its only a $4 \\times 4$ matrix and thus we only get back 4 singular values. We don't have to deal with any complex $\\sigma_i$, or throw any away, and most importantly we take the easier route computationally by working with a smaller matrix. We can also see that the eigenvectors we get through `np.linalg.eig()` make up the columns of the U matrix instead of V transpose, as was the case for $A^T A$. \n",
    "\n",
    "This is crucial to note because when it comes to computing the SVD of a matrix, the methods for obtaining the $\\mathbf{U}$ and $\\mathbf{V}^{\\mathrm{T}}$ matrices differ depending on the dimensionality of the original matrix. Specifically, if we start with a $m \\times n$ matrix $\\mathbf{A}$, where $m \\geq n$, then using $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}$ allows us to compute the columns of $V^T$ as the eigenvectors of the $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}$ matrix, while $U$ is computed with the method shown previously. \n",
    "\n",
    "On the other hand, when starting with a matrix $\\mathbf{A}$ where $n > m$, and we use $A A^T$, the methods for computing $\\mathbf{U}$ and $\\mathbf{V}^{\\mathrm{T}}$ swap. So the eigenvectors we compute make up the columns of our $U$ matrix, and thus we compute $V^T$ with the same methods shown previously for the $A^T A$ case when computing $U$. This subtle distinction can have significant implications for the efficiency and accuracy of computing a SVD, highlighting the importance of careful consideration of the matrix dimensions when compting a SVD.\n",
    "\n",
    "So that is why it is so important to pay attention to dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a455c24-6c14-414f-adba-ed454be49c81",
   "metadata": {},
   "source": [
    "## Reference Links\n",
    "\n",
    "https://pages.mtu.edu/~struther/Courses/OLD/Other/Sp2012/5627/SVD/Report/Singular%20Value%20Decomposition%20and%20its%20numerical%20computations.pdf\n",
    "\n",
    "https://youtu.be/mBcLRGuAFUk?si=3jrTEC-5jr2tvgYr\n",
    "\n",
    "https://youtu.be/rYz83XPxiZo?si=iWHsUpyBR9w4-CSZ\n",
    "\n",
    "https://en.wikipedia.org/wiki/Singular_value_decomposition#Applications_of_the_SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b432dea2-3b31-4cb3-9488-06407eed3e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
