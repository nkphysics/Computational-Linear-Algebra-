{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f6ab49",
   "metadata": {},
   "source": [
    "# Numerical Methods for Computing Eigenvalues\n",
    "\n",
    "### Matrix Similarity\n",
    "\n",
    "The first concept we will need to understand (if you don't already) is the concept of matrix similarity.\n",
    "When we say two matrices $A$ and $B$ are similar matrices it means that they have a very particular relationship.\n",
    "\n",
    "If an invertibe matrix $X$ exits such that the following relationship exists between $A_{n \\times n}$ and $B_{n \\times n}$ then $A$ and $B$ are *similar* matrices.\n",
    "\n",
    "$A = X^{-1} B X$\n",
    "\n",
    "If two matrices are similar they will share many of the same properties:\n",
    "\n",
    "- Rank\n",
    "- Determinant\n",
    "- Trace\n",
    "- ***Eigenvalues (MOST IMPORTANTLY)***\n",
    "- etc... (There are many more, see sources)\n",
    "\n",
    "Because similar matrices will share the same eigenvalues we can exploit the concept of matrix similarity to compute the eigenvalues of a matrix.\n",
    "\n",
    "So how do we go about finding some matrix $X$ or check if $A_{n \\times n}$ and $B_{n \\times n}$ are similar?\n",
    "\n",
    "### Schur Decomposition\n",
    "\n",
    "In order for us to leverage the knowledge of matrix similarity to compute eigenvalues we'll need to use the Schur matrix decomposition, using the $QR$ decomposition.\n",
    "\n",
    "Let's begin with the $QR$ decomposition...\n",
    "\n",
    "$A = QR$\n",
    "\n",
    "With $Q$ being an orthonormal matrix  recall that $Q^T Q = Q Q^T = I$, so $Q^T=Q^{-1}$.\n",
    "\n",
    "Let's then say $B = RQ$. From this we can play quite an clever trick with our $QR$ decomposition of $A$ and the porperties of orthonormal matrices.\n",
    "\n",
    "$B = RQ$\n",
    "\n",
    "$IB = IRQ$ \n",
    "\n",
    "$B = Q^{T}QRQ$\n",
    "\n",
    "$B = Q^{-1}QRQ$ (Remember $A = QR$)\n",
    "\n",
    "$B = Q^{-1}AQ$ (This is the Schur Decomposition)\n",
    "\n",
    "So with this we can say $B$ and $A$ are similar matrices. Further, by examining how we get the Schur decomposition we can see a way of producing an similar matrix for $A$ provided that we can perform a $QR$ decomposition for $A$.\n",
    "That is that we can just compute $RQ$ and we will have a similar matrix.\n",
    "\n",
    "To show or check that two similar matrices have the same eigenvalues, below we'll use `np.linalg.eig()` and `np.linalg.qr()` to confirm that $A$ and $B$ have the same eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2d1e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[41. 12. 87.]\n",
      " [62. 63. 98.]\n",
      " [72. 21. 61.]]\n",
      "Eigenvalues for A: \n",
      " [159.2103891 -29.8029942  35.5926051]\n",
      "RQ = B: \n",
      " [[169.57783173 -55.54164533  22.87003192]\n",
      " [ 36.12094529  17.86177242 -10.38914349]\n",
      " [ 31.584231   -23.65685678 -22.43960415]]\n",
      "Eigenvalues for B: \n",
      " [159.2103891 -29.8029942  35.5926051]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array(np.random.randint (0, 100, (3, 3)), dtype=np.float64)\n",
    "print(f\"A:\\n {A}\")\n",
    "A_eigvals, A_eigvecs = np.linalg.eig(A)\n",
    "print(f\"Eigenvalues for A: \\n {A_eigvals}\")\n",
    "\n",
    "Q, R = np.linalg.qr(A)\n",
    "B = np.dot(R, Q)\n",
    "print(f\"RQ = B: \\n {B}\")\n",
    "B_eigvals, B_eigvecs = np.linalg.eig(B)\n",
    "print(f\"Eigenvalues for B: \\n {B_eigvals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9548d52d",
   "metadata": {},
   "source": [
    "As you can clearly see the eigenvalues are the same for both $A$ and $B$.\n",
    "\n",
    "### The Rayleigh Quotient\n",
    "\n",
    "The following is defined as the Rayleigh Quotient.\n",
    "\n",
    "$r(\\vec{x}) \\equiv\n",
    "  \\begin{cases}\n",
    "    \\frac{\\langle x \\mid Ax \\rangle} {\\langle x \\mid x \\rangle} & \\quad \\text{}\\\\\n",
    "    \\frac{\\vec{x}^* A \\vec{x}} {\\vec{x}^* \\vec{x}} & \\quad \\text{Complex}\\\\\n",
    "    \\frac{\\vec{x}^T A \\vec{x}} {\\vec{x}^T \\vec{x}} & \\quad \\text{Real}\n",
    "  \\end{cases}\n",
    "$\n",
    "\n",
    "In the case of where $\\vec{x}$ is an eigenvector $(\\vec{v})$ of $A$, then the Rayeligh quotient will procude compute the corresponding eigenvalue for the eigenpair. Of course this will be useful in computing $\\lambda$s.\n",
    "\n",
    "$r(\\vec{v}) = \\lambda$\n",
    "\n",
    "The mathematical analysis to show this is beyond the scope of the focus of computing eigenvalues. If interested though for the sake of simplicity it's best to consider that $A$ is a strictly real and symmetric matrix, or if you want to consider the complex case assume $A$ is Hermitian to show this. However, this general idea behind the Rayleigh Quotiet can be applied to the complex non-Hermitian case, or in our case the real non-symmetric case. This can be seen in the code below using the same randomly generated $A$ from above and the computed eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d245b9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: \n",
      "[[41. 12. 87.]\n",
      " [62. 63. 98.]\n",
      " [72. 21. 61.]]\n",
      "Eigenvalues for A:\n",
      " [159.2103891 -29.8029942  35.5926051]\n",
      "Eigenvectors for A:\n",
      " [[-0.43021343 -0.75298887  0.23942701]\n",
      " [-0.76515787 -0.16862286 -0.96371169]\n",
      " [-0.47900922  0.63606139  0.11804441]]\n",
      "Eigenvalue 1: 159.21038910441362\n",
      "Eigenvalue 2: -29.80299420340707\n",
      "Eigenvalue 3: 35.592605098993445\n"
     ]
    }
   ],
   "source": [
    "print(f\"A: \\n{A}\")\n",
    "print(f\"Eigenvalues for A:\\n {A_eigvals}\")\n",
    "print(f\"Eigenvectors for A:\\n {A_eigvecs}\")\n",
    "# Eigenvectors from np.linalg.eig() comes in a form of a matrix which needs\n",
    "# to be transposed get at the eigenvectors in a loop\n",
    "\n",
    "def rquotient(vec, A):\n",
    "    top = np.dot(vec.T, np.dot(A, vec))\n",
    "    bottom = np.dot(vec.T, vec)\n",
    "    return top / bottom\n",
    "\n",
    "for num, i in enumerate(A_eigvecs.T):\n",
    "    eigval = rquotient(i, A)\n",
    "    print(f\"Eigenvalue {num + 1}: {eigval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360d568",
   "metadata": {},
   "source": [
    "As you can see from the output above passing each eigenvector into the Rayleigh Quotient function `rquotient` returns the expected eigenvalues for $A$ as computed before with `np.linalg.eig()`. This is what we can consider a baby method for computing $\\lambda$s. In reality the rayleigh quotient will be used, but as a part of more elaborate methods.\n",
    "\n",
    "### Computing $\\lambda$'s with $QR$\n",
    "\n",
    "The first method of computing $\\lambda$'s comes uses our knowledge of $QR$, matrix similarity, and how to get a similar matrix through the Schur Decomposition.\n",
    "\n",
    "We will start by computing a $QR$ decomposition for some matrix $A_0$ that can be decomposed with $QR$.\n",
    "\n",
    "$A_0 = Q_0 R_0$\n",
    "\n",
    "Then we will compute a similar matrix to $A_0$ using our $Q_0$ and $R_0$ matrices and call this new similar matrix $A_1$.\n",
    "\n",
    "$A_1 = R_0 Q_0$\n",
    "\n",
    "From this point the process will be repeated for $A_1$ (decompose into $Q_1 R_1$) to compute another new similar matrix $A_2$. We will continue through this process iteratively until we compute $A_n$ similar matrices.\n",
    "\n",
    "$A_1 = Q_1 R_1$\n",
    "\n",
    "$A_2 = R_1 Q_1$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$A_n = Q_n R_n$\n",
    "\n",
    "$A_{n+1} = R_n Q_n$\n",
    "\n",
    "What will happen throughout this process each similar matrix computed will get closer and closer, and ultimately converge to an upper triangular matrix (or at least in the numerical world, one with incredibly small values denoted as $\\epsilon$'s we can round to zero below the diagonal). Conveniently the $\\lambda$s will be on the diagonal.\n",
    "\n",
    "$A_n = \n",
    " \\begin{pmatrix}\n",
    "  \\lambda_1 & a_{1,2} & \\cdots & a_{1,m} \\\\\n",
    "  \\epsilon & \\lambda_2 & \\cdots & a_{2,m} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  0 & 0 & \\cdots & \\lambda_m\n",
    " \\end{pmatrix}$\n",
    " \n",
    " We put this into action with the following code for the same randomly generate $A$ matrix previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f1d24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_192 = \n",
      "[[ 1.59210389e+002 -8.75228331e+001 -3.14397403e+001]\n",
      " [ 2.60994360e-124  3.55926051e+001  2.30139517e+000]\n",
      " [ 5.41973865e-139 -1.93620954e-014 -2.98029942e+001]]\n",
      "A:\n",
      " [[41. 12. 87.]\n",
      " [62. 63. 98.]\n",
      " [72. 21. 61.]]\n",
      "Eignevalues: [159.21038910441362, 35.592605098993474, -29.80299420340708]\n",
      "# of iterations: 192\n"
     ]
    }
   ],
   "source": [
    "def makesimilar(A):\n",
    "    Q, R = np.linalg.qr(A)\n",
    "    B = np.dot(R, Q)\n",
    "    return B\n",
    "\n",
    "def eig_qr(A):\n",
    "    B = makesimilar(A)\n",
    "    iters = 0\n",
    "    leig = B[-1, -1]\n",
    "    diff = 1\n",
    "    while diff > 1e-32:\n",
    "        B = makesimilar(B)\n",
    "        iters += 1\n",
    "        diff = abs(leig - B[-1, -1])\n",
    "        leig = B[-1, -1]\n",
    "    print(f\"A_{iters} = \\n{B}\")\n",
    "    eigs = [B[i, i] for i in range(len(B))]\n",
    "    return eigs, iters\n",
    "\n",
    "eigs, iters = eig_qr(A)\n",
    "print(f\"A:\\n {A}\")\n",
    "print(f\"Eignevalues: {eigs}\")\n",
    "print(f\"# of iterations: {iters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8132a0e",
   "metadata": {},
   "source": [
    "As you can hopefully see we get the same $\\lambda$s as before when using `numpy.linalg.eig()` after computing 192 iterations of similar matrices. In `eig_qr()` we are computing similar matrices in the process of the QR algorithm until we reach a difference in values along the diagonal of the $n$th and $n - 1$ $A_n$'s on order of $10^{-32}$ (see the while loop).\n",
    "\n",
    "### Computing $\\lambda$'s with $QR$ + Shifts\n",
    "\n",
    "While computing $\\lambda'$s with $QR$ worked well, it took 192 iterations of computing similar matrices to converge to the point where the computer rounded the last value of the first column to zero. While we could lower our standards and compute less iterations, there is a faster way to converge to an upper triangular matrix and precision in out $\\lambda'$s with less iterations. That method is introducing shifts into the $QR$ method.\n",
    "\n",
    "\n",
    "There are 2 possible ways to start this method:\n",
    "\n",
    "1. Compute a QR decomposition with no shifts initially\n",
    "\n",
    "$A_0 = Q_0 R_0$\n",
    "\n",
    "$A_1 = R_0 Q_0$ (Like before)\n",
    "\n",
    "OR\n",
    "\n",
    "2. Shift A and then compute a QR decomposition for the shifted matrix for some value $s$\n",
    "\n",
    "$A_0 - sI = Q_0 R_0$\n",
    "\n",
    "Then reintroduce the shift when computing the similar matrix.\n",
    "\n",
    "$A_1 = R_0 Q_0 + sI$\n",
    "\n",
    "\n",
    "Either way for each subsequent similar matrix computation $(n >= 1)$ we'll use the 2nd method.\n",
    "\n",
    "$A_n - sI = Q_n R_n$\n",
    "\n",
    "$A_{n+1} = R_n Q_n + sI$\n",
    "\n",
    "But how do we choose a value for $s$? Should we even choose one at all or just use a random value?\n",
    "\n",
    "While you could use a random value it is unlikely to give too much of an efficiency improvement.\n",
    "\n",
    "To use this shifting method and see improved efficiency, we'll want to choose a value close to a $\\lambda$ or better yet use the **Rayleigh Quotient**. The motivation for this comes from a different, in my opinion, worse method to compute $\\lambda$s discussed later on, but the general idea is that we want to pick a value for $s$ close to a $\\lambda$.\n",
    "\n",
    "If we think about computing $\\lambda$s without shifts, each iteration of computing a similar $A_n$ matrix converges to an upper triangular matrix (roughly) with better approximations of the $\\lambda$s along the diagonal with each iteration. This means that the last value of each similar matrix $A_{m,m}$ is getting closer to an actual $\\lambda$ with each iteration. So with that in mind, we should choose a value along the diagonal of our similar matrix when starting each new iteration to be our shift $s$. We do that in the below code for the last value along the diagonal.\n",
    "\n",
    "$A_n = \n",
    " \\begin{pmatrix}\n",
    "  \\lambda_1 & a_{1,2} & \\cdots & a_{1,m} \\\\\n",
    "  \\epsilon & \\lambda_2 & \\cdots & a_{2,m} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  0 & 0 & \\cdots & s\n",
    " \\end{pmatrix}_{m \\times m}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96cea3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[41. 12. 87.]\n",
      " [62. 63. 98.]\n",
      " [72. 21. 61.]]\n",
      "Position: 1\n",
      "\n",
      "A_34 = \n",
      "[[ 1.59210389e+02 -8.75228331e+01 -3.14397403e+01]\n",
      " [ 4.22658806e-15  3.55926051e+01  2.30139517e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -2.98029942e+01]]\n",
      "Eignevalues: [159.21038910441368, 35.59260509899343, -29.802994203407092]\n",
      "# of iterations: 34\n",
      "Position: 2\n",
      "\n",
      "A_34 = \n",
      "[[ 1.59210389e+02 -8.75228331e+01 -3.14397403e+01]\n",
      " [ 4.22658806e-15  3.55926051e+01  2.30139517e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -2.98029942e+01]]\n",
      "Eignevalues: [159.21038910441368, 35.59260509899343, -29.802994203407092]\n",
      "# of iterations: 34\n",
      "Position: 3\n",
      "\n",
      "A_5 = \n",
      "[[ 1.59280078e+02 -8.74242377e+01  3.14415648e+01]\n",
      " [ 9.85953983e-02  3.55229163e+01 -2.27633279e+00]\n",
      " [-6.67396836e-41  6.35867329e-39 -2.98029942e+01]]\n",
      "Eignevalues: [159.28007787194323, 35.52291633146393, -29.802994203407092]\n",
      "# of iterations: 5\n"
     ]
    }
   ],
   "source": [
    "def eig_qrshifts(A, refeig=-1, sindex=-1, verbose=True):\n",
    "    B = makesimilar(A)\n",
    "    n = 0\n",
    "    leig = B[refeig, refeig]\n",
    "    diff = 1\n",
    "    while diff > 1e-32 and n < 100:\n",
    "        shift = np.identity(len(B)) * B[sindex, sindex]\n",
    "        C = B - shift\n",
    "        B = makesimilar(C) + shift\n",
    "        n += 1\n",
    "        diff = abs(leig - B[refeig, refeig])\n",
    "        leig = B[refeig, refeig]\n",
    "    if verbose is True:\n",
    "        print(f\"A_{n} = \\n{B}\")\n",
    "    eigs = [B[i, i] for i in range(len(B))]\n",
    "    return eigs, n\n",
    "\n",
    "print(f\"A:\\n {A}\")\n",
    "m, n = A.shape\n",
    "\n",
    "for i in range(0, n):\n",
    "    print(f\"Position: {i + 1}\\n\")\n",
    "    eigs, iters = eig_qrshifts(A, refeig=i)\n",
    "    print(f\"Eignevalues: {eigs}\")\n",
    "    print(f\"# of iterations: {iters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e0fdd",
   "metadata": {},
   "source": [
    "What we see in the above output for computing $\\lambda$s with the QR algorithm with shifts is that we are computing less similar matrices (doing less iterations) compared to just the QR algorithm. Also above, by using the `refeig` kwarg of `eig_qrshifts()` we can change the value along the diagonal we are using as our reference for comparing values with each iteration until we reach a difference between iterations of $<10^{-32}$. From using this we can see in the output that when we choose the shift to be the last value along the diagonal, setting the reference to the same last position along the diagonal converges to a precision on order of $<10^{-32}$ the fastest in only 5 iterations. When the reference is the 1st and 2nd value along the diagonal it takes 34.\n",
    "\n",
    "If the idea about choosing the shift to be the last value along the diagonal works well because with each iteration the values along the diagonal converge to be the $\\lambda$s, then couldn't we choose any value along the diagonal to be the shift $(s)$? The below code shows that choosing just any value along the diagonal as the shift doesn't work by iterating through the `refeig` and `sindex` (the shift index) kwargs of `eig_qrshifts()` keeping track of the number of iterations, stopping at 100 max iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6320878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m, n = A.shape\n",
    "miters = np.zeros((n, n))\n",
    "\n",
    "for i in range(0, n):\n",
    "    for j in range(0, n):\n",
    "        eigs, iters = eig_qrshifts(A, refeig=i, sindex=j, verbose=False)\n",
    "        miters[i, j] += iters\n",
    "\n",
    "plt.matshow(miters, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00ea4c",
   "metadata": {},
   "source": [
    "As we can see, using the last value along the diagonal as the shift $(s)$ converges the fastest for getting all $\\lambda$s to a precision on order $10^{-32}$. This is not an accident, rather it is because the last value along the diagonal is actually the **Rayleigh Quotient**, while the last column of each similar matrix is a better approximation of an eigenvector. To better understand why this is the case let's explore some other (in my opinion less effective) methods for computing $\\lambda$s.\n",
    "\n",
    "### Computing Max $\\lambda$ with Power Method (Von Mises Iteration Algorithm)\n",
    "\n",
    "The power method or Von Mises Iteration is a simple and old school way of computing the largest $\\lambda$ for a matrix $A$ using the Rayleigh Quotient. To use this method we'll choose a random vector $\\vec{x}_0$ and compute the following.\n",
    "\n",
    "$\\vec{x}_1 = \\frac{A \\vec{x}_0} {\\| A \\vec{x}_0 \\|}$\n",
    "\n",
    "The process will the repeat...\n",
    "\n",
    "$\\vec{x}_{n + 1} = \\frac{A \\vec{x}_n} {\\| A \\vec{x}_n \\|}$\n",
    "\n",
    "The idea here is that eventually $\\vec{x}$ will converge to an eigenvector $(\\vec{v})$.\n",
    "\n",
    "Then we can just use the Rayleigh Quotient to obtain the corresponding eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "482a0caf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues for A:\n",
      " [159.2103891 -29.8029942  35.5926051]\n",
      "Eigenvectors for A:\n",
      " [[-0.43021343 -0.75298887  0.23942701]\n",
      " [-0.76515787 -0.16862286 -0.96371169]\n",
      " [-0.47900922  0.63606139  0.11804441]] \n",
      "\n",
      "Max eigenvalue approximtion: 159.2103891055216\n",
      "Corresponding eigenvector: 16\n",
      "Iterations: 16\n"
     ]
    }
   ],
   "source": [
    "def powermethod(A):\n",
    "    x0 = np.random.rand(A.shape[1])\n",
    "    maxeig = 0\n",
    "    diff = 1\n",
    "    iters = 0\n",
    "    while diff >= 1e-8:\n",
    "        xn = np.dot(A, x0)\n",
    "        norm = np.linalg.norm(xn)\n",
    "        x0 = xn / norm\n",
    "        neweig = rquotient(x0, A)\n",
    "        diff = np.abs(maxeig - neweig)\n",
    "        maxeig = neweig\n",
    "        iters += 1\n",
    "    return maxeig, x0, iters\n",
    "\n",
    "meig = powermethod(A)[0]\n",
    "eigvec = powermethod(A)[1]\n",
    "iters = eigvec = powermethod(A)[2]\n",
    "print(f\"Eigenvalues for A:\\n {A_eigvals}\")\n",
    "print(f\"Eigenvectors for A:\\n {A_eigvecs} \\n\")\n",
    "print(f\"Max eigenvalue approximtion: {meig}\")\n",
    "print(f\"Corresponding eigenvector: {eigvec}\")\n",
    "print(f\"Iterations: {iters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f80b84",
   "metadata": {},
   "source": [
    "This works perfectly fine, and with more iterations the approximation of the eigenvector and eigenvalue will get better. The problem here though is that it only get us the largest $\\lambda$. Further it took 14 iterations to compute just 1 $\\lambda$ where as the QR method with shifts gave all the $\\lambda$.\n",
    "\n",
    "How do we get the rest of the $\\lambda$s? Short answer is introduce shifts\n",
    "\n",
    "### Computing $\\lambda$s with the Inverse Iteration and Rayleigh Iteration Methods\n",
    "\n",
    "The ***inverse iteration*** method for computing $\\lambda$s is effectively just power method with some added steps. The following formula is the method used for computing an approximate eigenvector as was the case with power method, but instead using a shift $s$ and inverting matrices (which is a problem in of itself).\n",
    "\n",
    "$\\vec{x}_{n + 1} = \\frac{(A - sI)^{-1}\\vec{x}_n} {\\| (A - sI)^{-1}\\vec{x}_n \\|}$\n",
    "\n",
    "The shift $s$ for inverse iteration is just a guess or estimate of for a $\\lambda$. By choosing a good guess for the shift this method will produce the eigenvector and eigenvalue after using the Rayleigh Quotient to give us the eigenpair. The big problem here is that we need a good guess, for the shift, that is close to one of the $\\lambda$s. Also we need to repeat this process for as many $\\lambda$s there will be for a matrix, so it isn't the most efficient. Turns out though that if you choose the shift (s) to be the Rayleigh Quotient of the approximate eigenvector produced with each iteration this process is made much more efficient. That is what is known as ***Rayleigh Iteration***, and can be seen in the following code. This is (where I surmise) the idea of shifting and using the Rayleigh Quotient for speeding up the QR algorithm came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b38fefb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues for A:\n",
      " [159.2103891 -29.8029942  35.5926051]\n",
      "Eigenvectors for A:\n",
      " [[-0.43021343 -0.75298887  0.23942701]\n",
      " [-0.76515787 -0.16862286 -0.96371169]\n",
      " [-0.47900922  0.63606139  0.11804441]] \n",
      "\n",
      "Guess 1:\n",
      "\n",
      "Eigenvalue approximtion: 159.21038910441362\n",
      "Corresponding eigenvector: [0.43021343 0.76515787 0.47900922]\n",
      "Iterations: 5\n",
      "Guess 2:\n",
      "\n",
      "Eigenvalue approximtion: -29.80299420340708\n",
      "Corresponding eigenvector: [-0.75298887 -0.16862286  0.63606139]\n",
      "Iterations: 5\n",
      "Guess 3:\n",
      "\n",
      "Eigenvalue approximtion: 35.59260509899346\n",
      "Corresponding eigenvector: [-0.23942701  0.96371169 -0.11804441]\n",
      "Iterations: 6\n"
     ]
    }
   ],
   "source": [
    "def eig_rayleigh(A, s):\n",
    "    x0 = np.random.rand(A.shape[1])\n",
    "    diff = 1\n",
    "    eig = 0\n",
    "    iters = 0\n",
    "    while diff >= 1e-16:\n",
    "        shift = np.identity(len(A)) * s\n",
    "        invert = np.linalg.inv(A - shift)\n",
    "        top = np.dot(invert, x0)\n",
    "        norm = np.linalg.norm(top)\n",
    "        x0 = np.divide(top, norm)\n",
    "        neweig = rquotient(x0, A)\n",
    "        diff = np.abs(eig - neweig)\n",
    "        eig = neweig\n",
    "        s = neweig\n",
    "        iters += 1\n",
    "    return eig, x0, iters\n",
    "\n",
    "\n",
    "print(f\"Eigenvalues for A:\\n {A_eigvals}\")\n",
    "print(f\"Eigenvectors for A:\\n {A_eigvecs} \\n\")\n",
    "for num, i in enumerate(A_eigvals):\n",
    "    print(f\"Guess {num + 1}:\\n\")\n",
    "    try:\n",
    "        guess = i - 0.99\n",
    "        result = eig_rayleigh(A, guess)\n",
    "        print(f\"Eigenvalue approximtion: {result[0]}\")\n",
    "        print(f\"Corresponding eigenvector: {result[1]}\")\n",
    "        print(f\"Iterations: {result[2]}\")\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Cannot invert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7dadd",
   "metadata": {},
   "source": [
    "As you can hopefully see we are able to compute two of the eigenvalues very well with >10 iterations. However the third $\\lambda$ cannot be computed do to not being able to compute the inverse $(A - sI)^{-1}$, which is another drawback of this method. Computing matrix inverses is tricky computationally/numerically and merits its own seperate discussion. So this (in my opinion) is not as effective as the QR algorithm or QR with shifts since we need to compute one $\\lambda$ at a time, need to pick a good guess, and hope that everything will invert. But, my motivation behind sharing this method and power method is that:\n",
    "\n",
    "1. They are both methods that will yield $\\lambda$s\n",
    "2. It may provide some understanding of were the use of shifting came from (idk it just connects for me in my mind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a6fd0",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://youtu.be/d32WV1rKoVk?si=J68lHihm_OBeYrrI\n",
    "\n",
    "https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/Index.html\n",
    "\n",
    "https://sci.utah.edu/~akil/docs/courses/2020fall/math6610/lec21.pdf\n",
    "\n",
    "https://sci.utah.edu/~akil/docs/courses/2020fall/math6610/lec20.pdf\n",
    "\n",
    "https://en.wikipedia.org/wiki/Eigenvalue_algorithm\n",
    "\n",
    "https://web.math.ucsb.edu/~padraic/ucsb_2013_14/math108b_w2014/math108b_w2014_lecture5.pdf\n",
    "\n",
    "https://youtu.be/Bt8o4Yn-71o?si=VDFbrGFBk7HdBirg\n",
    "\n",
    "https://johnfoster.pge.utexas.edu/numerical-methods-book/NumericalMethods.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9ab54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
