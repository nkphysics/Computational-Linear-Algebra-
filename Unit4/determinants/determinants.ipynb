{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42148e03-e0ce-4df8-a29f-f62f52dcfdd8",
   "metadata": {},
   "source": [
    "## Determinants using SVD\n",
    "\n",
    "Recall the singular value decomposition (check the SVD notebook in the unit 3 dir if you need a refresher).\n",
    "\n",
    "$A_{m \\times n} = U \\Sigma V^T$\n",
    "\n",
    "Every matrix can be decomposed with the SVD, and the properties of $U$, $\\Sigma$, and $V^T$ are often exploited to accomplish other operations or tasks. Despite the fact that a SVD can be computed for any $A_{m \\times n}$ we only really compute determinants for square matrices. For non-square matrices determinant computations may work, but for the sake of remaining focused on the numerical methods, is regarded as more of a not particularly useful pseudo-determinant without it being the actual pseudo-determinant (WARNING: this is a deep rabbit hole to go down). Because of this, we'll only consider computong determinants for square matrices.\n",
    "\n",
    "In the case of the determinant we can see the following:\n",
    "\n",
    "$\\det{(A)} = \\det{(U \\Sigma V^T)}$\n",
    "\n",
    "$\\det{(A)} = \\det{(U)} \\det{(\\Sigma)} \\det{(V^T)}$\n",
    "\n",
    "$U$ and $V$ are orthogonal so their determinants will be $\\pm 1$. Knowing if the determinant is positive or negative is not always important so we can just simplify using the absolute value.\n",
    "\n",
    "$|\\det{(A)}| = \\det{(\\Sigma)}$\n",
    "\n",
    "Since $\\Sigma$ is a square, diagonal matrix with the singular values running down the diagonal in decending order, computing $\\det{(\\Sigma)}$ just becomes the following:\n",
    "\n",
    "\n",
    "$|\\det{(A)}| = \\displaystyle \\prod_i{diag(\\Sigma)_i} = \\displaystyle \\prod_{i=1}^n \\sigma_i$\n",
    "\n",
    "Where $n$ is the total number of singular values.\n",
    "\n",
    "So this won't quite give us the actual determinant, but will give us the absolute value of the determinant, which in many cases will be useful enough in of itself. \n",
    "\n",
    "The following code will test this method against `np.linalg.det()` which uses a different backend method for computing the determinant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03337fe2-8a9c-45b5-a373-6f6e6e726bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-994  169  349  986 -739 -226  495 -666]\n",
      " [-457  664 -727  825  263 -923 -201  355]\n",
      " [-278 -739  274 -246  837 -430  545 -884]\n",
      " [ 130  765  -78  -28  748  103 -453  888]\n",
      " [ 728 -848 -321  463 -122 -413  125  707]\n",
      " [-965  672    2 -940   77  396  935   30]\n",
      " [ 607  539  738   49 -613  510 -441 -987]\n",
      " [ -86 -769 -634 -763 -180 -945  -25  176]]\n",
      "\n",
      "Singular Value Decomposition Determinant:\n",
      " 3.3031297555860534e+24\n",
      "\n",
      "np.linalg.det() function result: \n",
      " -3.303129755586057e+24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "A = np.random.randint(-1000, 1001, size=(8, 8))# Random 8x8 matrix\n",
    "\n",
    "# Compute the SVD of each matrix\n",
    "U_A, s_A, Vh_A = np.linalg.svd(A)\n",
    "det_A_svd = np.prod(s_A)  # Compute determinant from singular values\n",
    "det_A_np = np.linalg.det(A)   # Compare with numpy determinant function\n",
    "\n",
    "print(f\"A:\\n {A}\\n\")\n",
    "print(f\"Singular Value Decomposition Determinant:\\n {det_A_svd}\\n\")\n",
    "print(f\"np.linalg.det() function result: \\n {det_A_np}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29618dfe-8935-4fea-a642-8d37054db8e5",
   "metadata": {},
   "source": [
    "From this we can also see that both values for $|\\det{(A)}|$ from both methods are not perfectly identical. This is due to the difference in methods used and the nature of numerical mathematics and the error associated with it. Neverthless though we do get respectable agreement between both value's significant figures despite the fact that the value is of a high order.\n",
    "\n",
    "## Determinants with Eigenvalues (kinda)\n",
    "\n",
    "Let's suppose that we want to use eigenvalues instead of singular values. After all, a similar line of reasoning should allow that to work as seen with the following using the eigen-decomposition $A = Q \\Lambda Q^T$:\n",
    "\n",
    "$\\det{(A)} = \\det{(Q \\Lambda Q^T)}$\n",
    "\n",
    "$\\det{(A)} = \\det{(Q)} \\det{(\\Lambda)} \\det{(Q^T)}$\n",
    "\n",
    "Recall $Q$ is also orthogonal so $\\det{(Q)} = \\pm 1$. \n",
    "Also recall that we can have negative eigenvalues.\n",
    "\n",
    "$\\det{(A)} = \\det{(\\Lambda)}$\n",
    "\n",
    "$\\det{(A)} = \\displaystyle \\prod_i{diag(\\Lambda)_i} = \\displaystyle \\prod_{i=1}^n \\lambda_i$\n",
    "\n",
    "Since we can only compute determinants for square matrices then this should work, right?\n",
    "\n",
    "As can be seen in the following code block, eigenvalues can be complex, which leads to a complex determinant. Remember that not all square matrices have strictly real eigenvalues. A problem also arises with complex eigenvalues which is complex eigenvectors. Recall that eigenvectors make up the $Q$ matrix of the eigendecomposition. If the eigenvectors are complex then $\\det(q) \\neq \\pm 1$, which makes our whole idea of using the eigendecomposition problematic. This among other considerations regarding computation of eigenvalues makes using eigenvalues for computing determinants less than ideal. However, what we can see in the following code block is that if we have complex eigenvalues, the real part of the product of the eigenvalues agrees very well with the actual determinant computed earlier.\n",
    "\n",
    "$\\det{(A)} = \\Re{ \\bigg (\\displaystyle \\prod_{i=1}^n \\lambda_i \\bigg )}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60745be5-ff0f-4deb-a963-46eb75baa02d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det(Q): (6.306173644480768e-18+0.06208828425015045j)\n",
      "det(Q inverse): (-9.65955213048664e-16-16.10609814842124j)\n",
      "Product of eigenvalues: (-3.3031297555860523e+24-230060587.5322334j)\n"
     ]
    }
   ],
   "source": [
    "eigvals, eigvecs = np.linalg.eig(A)\n",
    "print(f\"det(Q): {np.linalg.det(eigvecs)}\")\n",
    "print(f\"det(Q inverse): {np.linalg.det(np.linalg.inv(eigvecs))}\")\n",
    "eig_det = np.prod(eigvals)\n",
    "print(f\"Product of eigenvalues: {eig_det}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b8056-843d-4d84-8246-3b7b34bf1e36",
   "metadata": {},
   "source": [
    "With we consider the case of $A^T A$ we will get a positive definate matrix which will have strictly real eigenvalues. Computing the same determinants for $Q$, $\\Lambda$, and $Q^{-1}$ in the following block, we can see that the original idea holds and we get a strictly real determinant for each matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6ee9586-6b43-4f4d-bbc7-9b3716bb3060",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det(Q): 0.9999999999999996\n",
      "det(Q inverse): 1.0000000000000004\n",
      "Product of eigenvalues: 1.0910666182237947e+49\n"
     ]
    }
   ],
   "source": [
    "ATA = np.dot(A.T, A)\n",
    "eigvals, eigvecs = np.linalg.eig(ATA)\n",
    "print(f\"det(Q): {np.linalg.det(eigvecs)}\")\n",
    "print(f\"det(Q inverse): {np.linalg.det(np.linalg.inv(eigvecs))}\")\n",
    "eig_det = np.prod(eigvals)\n",
    "print(f\"Product of eigenvalues: {eig_det}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d6dc2-1085-4ec6-a668-6b101fe06109",
   "metadata": {},
   "source": [
    "## Determinants with LU Decomposition\n",
    "\n",
    "More commonly the $LU$ decomposition is used to compute determinants since the decomposition is simpler than using $\\sigma_i$'s or $\\lambda_i$'s. Computing the determinant with $LU$ involves just computing the products of $diag(L)$ and $diag(U)$, plus it gives the full $\\det(A)$ instead of $|\\det(A)|$ as is the case with $\\sigma_i$'s.\n",
    "\n",
    "$\\det(A) = \\det(LU) = \\det(L) \\det(U)$\n",
    "\n",
    "$\\det(A) = \\displaystyle \\prod_{i} l_{ii} \\displaystyle \\prod_{i} u_{ii}$\n",
    "\n",
    "If you need to convince yourself of this just take a $3 \\times 3$ upper and lower triangular matrix and compute the determinant for them, and you'll find that everything off the diagonal at one point or another ends up getting multiplied by zero except for the product of the values along the diagonal.\n",
    "\n",
    "Computation of the determinant with the diagonals of $LU$ for the same $A$ matrix as before, can be seen in the following code block using `scipy.linalg.lu()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7a9c1dc-8c55-4bfc-aa44-6f5e0b143621",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product of Values from diagonal of L and U: 3.3031297555860566e+24\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "p, L, U = scipy.linalg.lu(A.copy())\n",
    "det = np.prod(L.diagonal()) * np.prod(U.diagonal())\n",
    "print(f\"Product of Values from diagonal of L and U: {det}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6bcaa7-6bc7-40c6-95b5-759dd4ac1b4a",
   "metadata": {},
   "source": [
    "So the above result agrees very well with the previous $\\det(A)$ value obtained with `np.linalg.det()`. Plus the $LU$ decomposition has $O(n^3)$ time complexity, so while it isn't the fastest method, it still just relies on matrix multiplication and is relatively respectable. Matrix multiplication can be spead up with the *Strassen Algorithm* which efectively follows the same process we took here, however has closer to $O(n^{2.5})$ time complexity (THIS IS NOT EXACT, DON\"T QUOTE ME ON IT, the point is to say Strassen's algorithm for matrix multiplication is faster). I don't have any notes on the *Strassen Algorithm* yet, so I may need to create those in the future.\n",
    "\n",
    "Also do note `scipy.linalg.lu()` uses a permutation matrix, hence `p, L, U = scipy.linalg.lu(A.copy())` which is another way of doing a $LU$ decomposition that differs slightly from my notes on the $LU$ decomposition, however the same idea will apply to both cases. When using a permutation matrix, like in this case with the scipy function, we get the $\\pm 1$ from the determinant of the permutation matrix. So in the above code we are only guaranteed $|\\det (A)|$ like in the case of using SVD.\n",
    "\n",
    "## Determinants with Cholesky Decomposition\n",
    "\n",
    "If we consider a positive definite matrix such as $A^T A$ we can use the Cholesky decomposition which gives us the benefit of only working with one matrix $L$, the Cholesky factor. Recall:\n",
    "\n",
    "$A = L L^T$\n",
    "\n",
    "Generally speaking, $A^T A$ is positive definite so we can test this out using the `np.linalg.cholesky()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17f170dd-ab06-4db5-bd34-6748d32388d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.linalg.det() function result: \n",
      " 1.0910666182238005e+49\n",
      "\n",
      "Determinant from Cholesky Decomposition: \n",
      "1.0910666182238023e+49\n"
     ]
    }
   ],
   "source": [
    "# Use A transpose A computed from before with the eigenvalues\n",
    "det_ATA = np.linalg.det(ATA)\n",
    "print(f\"np.linalg.det() function result: \\n {det_ATA}\\n\")\n",
    "\n",
    "# Compute the determinant with cholesky\n",
    "chol_factor = np.linalg.cholesky(ATA)\n",
    "chol_det = np.prod(chol_factor.diagonal()) ** 2\n",
    "print(f\"Determinant from Cholesky Decomposition: \\n{chol_det}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0138d919-3022-4d2f-9b95-c740e7b1aac0",
   "metadata": {},
   "source": [
    "## Bareiss Algorithm\n",
    "\n",
    "The last method I want to examine is the *Bareiss Algorithm*, which can be used to compute integer determinants for matrices of only real integer values. What's meant by the term \"integer determinants\" is that no floats will be used, and only mathematics operations will be conducted with integer values. Its named after Erwin Bareiss who formally proposed the algorithm in 1968 in a paper called *Sylvester’s identity and multistep integer-preserving Gaussian elimination*. \n",
    "\n",
    "This is a bit of a weird and special case, but a cool algorithm that I thought would be interesting to explore with the topic of numerical methods for computing determinants. \n",
    "\n",
    "Practically speaking, the Bareiss Algorithm appears to be to be marginally better (from the sources I've read through) compared to using $LU$ with Gaussian elimination since Gaussian Elimination introduces division, which, therefore requires the use of floating point values. Keeping a consistent integer data type and allowing for arithmetic operations to be done strictly using integers, clearly, in theory, allows the Bareiss Algorithm to be more performant than $LU$ with Gaussian Elimination because of this.\n",
    "\n",
    "The steps for performing the Bareiss Algorithm are not super straight forward, but the following is my best description of the steps to perform the Bareiss Algorithm:\n",
    "\n",
    "1. Start with a sign factor $=1$\n",
    "\n",
    "2. Iterate through columns: For each column index $k = 0, 1, ..., n-2$ and search for a zero on the diagonal. If $A[k][k] = 0$, then search for a non-zero element in the same column below the current row. Perform a row swap to eliminate the zero, i.e., interchange rows $k$ and $l$, and multiply the sign factor every time we swap rows by $-1$.\n",
    "$$\\begin{pmatrix} A[k] \\\\ A[l] \\end{pmatrix} \\mapsto \\begin{pmatrix} A[l] \\\\ A[k] \\end{pmatrix}.$$\n",
    "\n",
    "3. Eliminate values below the diagonal: For each column index $i > k$ and each column index $j \\geq k+1$, compute:\n",
    "$$a_{ij} = a_{kk} \\cdot a_{ij} - a_{ik} \\cdot a_{kj}$$\n",
    "\n",
    "4. Divide rows below by the pivot element. If $k > 0$, then for each row index $i > k$, divide all elements in that row by the pivot element.\n",
    "\n",
    "5. Repeat steps 2 and 3 until the top-right corner is reached\n",
    "\n",
    "6. Multiply the sign factor and last diagonal element\n",
    "\n",
    "Don't take this decription as 100% accurate. Again, this process is still not entirely clear to me, but I wanted to provide a description to be best of my current understanding as of writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8966fe6-7aba-4f76-83ca-efb3a2e3fb8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant from np.linalg.det(): \n",
      "-20335341030.00003\n",
      "\n",
      "Determinant from Bareiss Algorithm: \n",
      "-20335341030\n"
     ]
    }
   ],
   "source": [
    "def bareiss(A: np.ndarray):\n",
    "    # Get the dimensions of the matrix\n",
    "    m, n = np.shape(A)\n",
    "    sign = 1\n",
    "    \n",
    "    # Check if the matrix is square; raise an error if not\n",
    "    if m != n:\n",
    "        raise TypeError(\"Matrix is not square\")\n",
    "    \n",
    "    # Initialize the sign factor to 1 (or -1 for a row swap)\n",
    "    for k in range(n - 1):\n",
    "        # Iterate through columns\n",
    "        if A[k][k] == 0:\n",
    "            # If a zero is on the diagonal\n",
    "            for l in range(k + 1, n):\n",
    "                if A[l][k] != 0:\n",
    "                    # Swap the two rows\n",
    "                    A[l], A[k] = A[k], A[l]\n",
    "                    # Update the sign factor (since we swapped a row)\n",
    "                    sign = -sign\n",
    "                    break\n",
    "            # If we reached this point and didn't swap rows, the matrix is singular\n",
    "            # so the determinant of the matrix will be zero\n",
    "            if (l == n - 1):\n",
    "                return 0            \n",
    "        # Subtract multiples of one column from others to eliminate \n",
    "        # columns below the diagonal\n",
    "        for i in range(k + 1, n):\n",
    "            for j in range(k + 1, n):\n",
    "                A[i][j] = A[k][k] * A[i][j] - A[i][k] * A[k][j]\n",
    "                # If we're not at the top row yet, divide rows below by the pivot element\n",
    "                if k != 0:\n",
    "                    A[i][j] = A[i][j] // A[k - 1][k - 1]\n",
    "    # The determinant the sign factor times the last diagonal element\n",
    "    return (sign * A[n - 1][n - 1])\n",
    "\n",
    "B = np.random.randint(-20, 21, size=(8, 8), dtype=np.int64)\n",
    "npdet = np.linalg.det(B)\n",
    "print(f\"Determinant from np.linalg.det(): \\n{npdet}\\n\")\n",
    "bareiss_det = bareiss(B)\n",
    "print(f\"Determinant from Bareiss Algorithm: \\n{bareiss_det}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28e5c1-7814-4f44-8e43-c7aedf284536",
   "metadata": {},
   "source": [
    "From the code block above using the Bareiss Algorithm we strictly get an integer value, compared to `np.linalg.det()` which still returns a float. Also, do note I had to generate a different matrix $B$ for this example since I kept running into overflow warnings in the above code, so the above implementation may not be the best for use in real world applications.\n",
    "\n",
    "## Citations\n",
    "\n",
    "Bareiss, Erwin H. (1968), \"Sylvester's Identity and multistep integer-preserving Gaussian elimination\" (PDF), Mathematics of Computation, 22 (103): 565–578, https://doi.org/10.1090/S0025-5718-1968-0226829-0, JSTOR 2004533\n",
    "\n",
    "## Reference Links\n",
    "\n",
    "https://math.stackexchange.com/questions/3222706/singular-value-decomposition-determinant\n",
    "\n",
    "MIT 18.06 Linear Algebra, Spring 2005, By Gilbert Strang, Lecture 18:\n",
    "https://youtu.be/srxexLishgY?si=W7L50FNFTUrN8m7k\n",
    "\n",
    "MIT 18.06 Linear Algebra, Spring 2005, By Gilbert Strang, Lecture 19:\n",
    "https://youtu.be/23LLB9mNJvc?si=2ebB6UGuUosCOjRG\n",
    "\n",
    "Bariess Paper can also be found here (but I think it'll take you to the same place as if you were to use the doi):\n",
    "https://www.ams.org/journals/mcom/1968-22-103/S0025-5718-1968-0226829-0/\n",
    "\n",
    "https://en.wikipedia.org/wiki/LU_decomposition\n",
    "\n",
    "https://stackoverflow.com/questions/27003062/fastest-algorithm-for-computing-the-determinant-of-a-matrix\n",
    "\n",
    "https://en.wikipedia.org/wiki/Bareiss_algorithm\n",
    "\n",
    "https://cs.stackexchange.com/questions/124759/determinant-calculation-bareiss-vs-gauss-algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
